"""
CogMind v3.2.1 — MAX+++ AutoLearner + Persistent DiskKV,
Integrated "Engineer-Mind" + Nano Constitution + SafeFS
Perception/Compression + Safe Self-Plugins (CLI + optional Tkinter GUI)
"""

from __future__ import annotations

__version__ = "3.2.1"

import argparse
import ast
import base64
import builtins
import collections
import hashlib
import importlib.util
import json
import logging
import os
import re
import shutil
import subprocess
import sys
import tempfile
import uuid
from collections.abc import Callable, Iterable
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from typing import Any, Protocol

logger = logging.getLogger("nano")

# ============================
#        Limits (safe)
# ============================

# env-ით მართვადი ლიმიტები; ნაგულისხმევად ~16MB
MAX_STDIN_BYTES = int(os.environ.get("COGMIND_MAX_STDIN_BYTES", str(16 * 1024 * 1024)))
MAX_INGEST_BYTES = int(
    os.environ.get("COGMIND_MAX_INGEST_BYTES", str(16 * 1024 * 1024))
)

# ============================
#       Data models
# ============================


@dataclass
class Thought:
    role: str
    content: str
    ts: datetime = field(default_factory=datetime.utcnow)
    score: float = 0.0


@dataclass
class Episode:
    id: str
    topic: str
    steps: list[Thought] = field(default_factory=list)
    started_at: datetime = field(default_factory=datetime.utcnow)
    finished_at: datetime | None = None

    def add(self, t: Thought) -> None:
        self.steps.append(t)

    def text(self) -> str:
        return "\n".join(f"[{t.role}] {t.content}" for t in self.steps)


@dataclass
class Goal:
    text: str
    done: bool = False
    success_criteria: list[str] = field(default_factory=list)
    deadline: datetime | None = None


@dataclass
class PlanStep:
    id: str
    desc: str
    accept_criteria: list[str]
    depends_on: list[str] = field(default_factory=list)
    status: str = "PENDING"  # PENDING | RUNNING | OK | FAIL
    evidence: dict[str, Any] = field(default_factory=dict)


# ============================
#        Memory contract
# ============================


class MemoryAdapter(Protocol):
    def store(
        self, kind: str, payload: dict[str, Any], *, ttl: timedelta | None = None
    ) -> str: ...
    def recall(self, record_id: str) -> dict[str, Any] | None: ...
    def search(
        self, query: str, *, k: int = 8, kind: str | None = None
    ) -> list[dict[str, Any]]: ...
    def summarize(
        self, items: Iterable[dict[str, Any]], *, max_chars: int = 1000
    ) -> str: ...


class InMemoryKV(MemoryAdapter):
    def __init__(self) -> None:
        self._store: dict[str, dict[str, Any]] = {}

    def store(
        self, kind: str, payload: dict[str, Any], *, ttl: timedelta | None = None
    ) -> str:
        rid = str(uuid.uuid4())
        record = dict(payload)
        record.update(
            {
                "id": rid,
                "kind": kind,
                "created_at": datetime.utcnow().isoformat(),
                "ttl": ttl.total_seconds() if ttl else None,
            }
        )
        self._store[rid] = record
        return rid

    def recall(self, record_id: str) -> dict[str, Any] | None:
        return self._store.get(record_id)

    def search(
        self, query: str, *, k: int = 8, kind: str | None = None
    ) -> list[dict[str, Any]]:
        q = (query or "").lower()
        items = [
            {"id": rid, **data}
            for rid, data in self._store.items()
            if (kind is None or data.get("kind") == kind)
        ]

        def score(it: dict[str, Any]) -> float:
            text = json.dumps(it, ensure_ascii=False).lower()
            base = 1.0 if q in text else 0.0
            terms = set([w for w in re.split(r"[^\w]+", q) if w])
            text_terms = set([w for w in re.split(r"[^\w]+", text) if w])
            inter = len(terms & text_terms)
            return base + min(1.0, inter / 3.0)

        items.sort(key=score, reverse=True)
        return items[:k]

    def summarize(
        self, items: Iterable[dict[str, Any]], *, max_chars: int = 1000
    ) -> str:
        parts: list[str] = []
        used = 0
        for it in items:
            chunk = (
                it.get("text")
                or it.get("content")
                or json.dumps(it, ensure_ascii=False)
            )
            if used + len(chunk) > max_chars:
                chunk = chunk[: max(0, max_chars - used)]
            parts.append(chunk)
            used += len(chunk)
            if used >= max_chars:
                break
        return "\n---\n".join(parts)


# ============================
#           SafeFS
# ============================


class SafeFS:
    """
    Enforces all writes under a single root. Blocks attempts outside.
    """

    def __init__(self, root_dir: str, constitution_path: str | None = None) -> None:
        self.root = os.path.abspath(root_dir)
        self.constitution_path = (
            os.path.abspath(constitution_path) if constitution_path else None
        )
        os.makedirs(self.root, exist_ok=True)

    def _abs(self, path: str) -> str:
        if not os.path.isabs(path):
            path = os.path.join(self.root, path)
        p = os.path.abspath(path)
        if not (p == self.root or p.startswith(self.root + os.sep)):
            raise PermissionError("path_outside_safe_root")
        if self.constitution_path and os.path.abspath(p) == self.constitution_path:
            raise PermissionError("constitution_is_immutable")
        return p

    def ensure_dir(self, rel_or_abs: str) -> str:
        p = self._abs(rel_or_abs)
        os.makedirs(p, exist_ok=True)
        return p

    def write_text(self, rel_or_abs: str, text: str) -> str:
        p = self._abs(rel_or_abs)
        os.makedirs(os.path.dirname(p), exist_ok=True)
        with open(p, "w", encoding="utf-8") as f:
            f.write(text)
            f.flush()
            try:
                os.fsync(f.fileno())
            except Exception:
                pass
        return p

    def listdir(self, rel_or_abs: str) -> list[str]:
        p = self._abs(rel_or_abs)
        return os.listdir(p)

    def join(self, *parts: str) -> str:
        return self._abs(os.path.join(*parts))


# ============================
#        Persistent DiskKV
# ============================


class DiskKV(MemoryAdapter):
    """
    Simple JSONL-backed key-value store with in-memory index.
    File is always kept inside SafeFS root (path is pre-validated by caller).
    Schema per line: {"id": "...", "kind": "...", ...}
    """

    def __init__(self, jsonl_path: str) -> None:
        self.path = os.path.abspath(jsonl_path)
        os.makedirs(os.path.dirname(self.path), exist_ok=True)
        self._store: dict[str, dict[str, Any]] = {}
        # backup existing file to .bak
        if os.path.isfile(self.path):
            try:
                shutil.copy2(self.path, self.path + ".bak")
            except Exception:
                pass
        self._load_existing()

    def _load_existing(self) -> None:
        if not os.path.isfile(self.path):
            with open(self.path, "w", encoding="utf-8") as _:
                pass
            return
        try:
            with open(self.path, encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        rec = json.loads(line)
                        rid = rec.get("id") or str(uuid.uuid4())
                        rec["id"] = rid
                        self._store[rid] = rec
                    except Exception:
                        continue
        except Exception:
            # try from backup
            bak = self.path + ".bak"
            if os.path.isfile(bak):
                try:
                    with open(bak, encoding="utf-8") as f:
                        for line in f:
                            line = line.strip()
                            if not line:
                                continue
                            try:
                                rec = json.loads(line)
                                rid = rec.get("id") or str(uuid.uuid4())
                                rec["id"] = rid
                                self._store[rid] = rec
                            except Exception:
                                continue
                except Exception:
                    pass

    def _append(self, record: dict[str, Any]) -> None:
        try:
            with open(self.path, "a", encoding="utf-8") as f:
                f.write(json.dumps(record, ensure_ascii=False) + "\n")
                f.flush()
                try:
                    os.fsync(f.fileno())
                except Exception:
                    pass
        except Exception:
            # Last resort: swallow to avoid crashing core run; data stays in RAM.
            pass

    # ---- MemoryAdapter API ----
    def store(
        self, kind: str, payload: dict[str, Any], *, ttl: timedelta | None = None
    ) -> str:
        rid = str(uuid.uuid4())
        record = dict(payload)
        record.update(
            {
                "id": rid,
                "kind": kind,
                "created_at": datetime.utcnow().isoformat(),
                "ttl": ttl.total_seconds() if ttl else None,
            }
        )
        self._store[rid] = record
        self._append(record)
        return rid

    def recall(self, record_id: str) -> dict[str, Any] | None:
        return self._store.get(record_id)

    def search(
        self, query: str, *, k: int = 8, kind: str | None = None
    ) -> list[dict[str, Any]]:
        q = (query or "").lower()
        items = [
            rec
            for rec in self._store.values()
            if (kind is None or rec.get("kind") == kind)
        ]

        def score(it: dict[str, Any]) -> float:
            text = json.dumps(it, ensure_ascii=False).lower()
            base = 1.0 if q in text else 0.0
            terms = set([w for w in re.split(r"[^\w]+", q) if w])
            text_terms = set([w for w in re.split(r"[^\w]+", text) if w])
            inter = len(terms & text_terms)
            return base + min(1.0, inter / 3.0)

        items.sort(key=score, reverse=True)
        return items[:k]

    def summarize(
        self, items: Iterable[dict[str, Any]], *, max_chars: int = 1000
    ) -> str:
        parts: list[str] = []
        used = 0
        for it in items:
            chunk = (
                it.get("text")
                or it.get("content")
                or json.dumps(it, ensure_ascii=False)
            )
            if used + len(chunk) > max_chars:
                chunk = chunk[: max(0, max_chars - used)]
            parts.append(chunk)
            used += len(chunk)
            if used >= max_chars:
                break
        return "\n---\n".join(parts)

    # ---- Utility for exports ----
    def iter_items(self, kind: str | None = None) -> Iterable[dict[str, Any]]:
        for rec in self._store.values():
            if kind is None or rec.get("kind") == kind:
                yield rec


# ============================
#           Tools / Skills
# ============================


class Tool(Protocol):
    name: str
    description: str

    def use(self, **kwargs: Any) -> dict[str, Any]: ...


class ToolsRegistry:
    def __init__(self) -> None:
        self._tools: dict[str, Tool] = {}

    def register(self, tool: Tool) -> None:
        if tool.name in self._tools:
            raise ValueError(f"Tool already registered: {tool.name}")
        self._tools[tool.name] = tool

    # NEW: upsert — არასებული ჩანაწერის ჩანაცვლება (hot-reload skills)
    def register_or_replace(self, tool: Tool) -> None:
        self._tools[tool.name] = tool

    def call(self, name: str, **kwargs: Any) -> dict[str, Any]:
        if name not in self._tools:
            sname = f"skill:{name}"
            if sname in self._tools:
                return self._tools[sname].use(**kwargs)
            raise KeyError(f"Unknown tool: {name}")
        return self._tools[name].use(**kwargs)

    def list(self) -> builtins.list[str]:
        return sorted(self._tools.keys())


class PyLintLite:
    name = "pylint_lite"
    description = "Very small linter: checks banned calls, too-long lines."

    def use(self, **kwargs: Any) -> dict[str, Any]:
        code = kwargs.get("code") or ""
        banned = ["os.system", "subprocess.Popen", "eval(", "exec("]
        warnings = []
        for b in banned:
            if b in code:
                warnings.append(f"banned:{b}")
        long_lines = [
            i + 1 for i, line in enumerate(code.splitlines()) if len(line) > 120
        ]
        if long_lines:
            warnings.append(f"long_lines:{long_lines[:10]}")
        return {"ok": len(warnings) == 0, "warnings": warnings}


class SandboxRunner:
    name = "sandbox_runner"
    description = "Safe, allowlisted subprocess execution (no internet)."
    ALLOW_CMDS = {"python"}

    def use(self, **kwargs: Any) -> dict[str, Any]:
        code = kwargs.get("code", "")
        args = kwargs.get("args", [])
        time_limit = int(kwargs.get("time_limit", 5))
        max_output_bytes = int(kwargs.get("max_output_bytes", 4000))
        if not isinstance(args, list):
            args = [str(args)]
        with tempfile.TemporaryDirectory() as td:
            py = os.path.join(td, "snippet.py")
            with open(py, "w", encoding="utf-8") as f:
                f.write(code)
            # FORCE UTF-8 for child python
            cmd = ["python", "-X", "utf8", "-I", "-S", py] + [str(a) for a in args]
            if cmd[0] not in self.ALLOW_CMDS:
                return {"ok": False, "error": "command_not_allowed"}
            try:
                logger.debug("Sandbox exec: %s", cmd)
                env = dict(os.environ)
                env.setdefault("PYTHONUTF8", "1")
                env.setdefault("PYTHONIOENCODING", "utf-8")
                proc = subprocess.run(
                    cmd, capture_output=True, text=True, timeout=time_limit, env=env
                )

                def _tail(s: str) -> str:
                    b = (s or "").encode("utf-8", errors="ignore")
                    if len(b) <= max_output_bytes:
                        return s or ""
                    return b[-max_output_bytes:].decode("utf-8", errors="ignore")

                out = _tail(proc.stdout)
                err = _tail(proc.stderr)
                return {
                    "ok": proc.returncode == 0,
                    "rc": proc.returncode,
                    "stdout": out,
                    "stderr": err,
                }
            except subprocess.TimeoutExpired:
                return {"ok": False, "error": "timeout"}
            except Exception as e:
                return {"ok": False, "error": str(e)}


# ============================
#      Constitution (immutable)
# ============================


@dataclass
class Constitution:
    allowed_imports: set[str] = field(
        default_factory=lambda: {
            "math",
            "statistics",
            "random",
            "re",
            "json",
            "typing",
            "dataclasses",
            "functools",
            "itertools",
            "collections",
        }
    )
    banned_imports: set[str] = field(
        default_factory=lambda: {
            "os",
            "sys",
            "subprocess",
            "socket",
            "http",
            "urllib",
            "ssl",
            "ctypes",
            "multiprocessing",
            "threading",
            "pathlib",
            "shutil",
        }
    )
    banned_calls: set[str] = field(
        default_factory=lambda: {"eval", "exec", "__import__", "open"}
    )
    max_top_level_statements: int = 200
    self_improvement: bool = True
    allow_autolearn: bool = True
    safe_root: str | None = None

    @classmethod
    def load(cls, path: str | None) -> Constitution:
        if path and os.path.isfile(path):
            try:
                with open(path, encoding="utf-8") as f:
                    data = json.load(f)
                return cls(
                    allowed_imports=set(data.get("allowed_imports", []))
                    or cls().allowed_imports,
                    banned_imports=set(data.get("banned_imports", []))
                    or cls().banned_imports,
                    banned_calls=set(data.get("banned_calls", []))
                    or cls().banned_calls,
                    max_top_level_statements=int(
                        data.get("max_top_level_statements", 200)
                    ),
                    self_improvement=bool(data.get("self_improvement", True)),
                    allow_autolearn=bool(data.get("allow_autolearn", True)),
                    safe_root=data.get("safe_root") or None,
                )
            except Exception:
                return cls()
        return cls()

    def check_code(self, code: str) -> list[str]:
        violations: list[str] = []
        try:
            tree = compile(code, "<skill>", "exec", ast.PyCF_ONLY_AST)
        except SyntaxError as e:
            return [f"syntax_error:{e}"]
        body = getattr(tree, "body", [])
        if len(body) > self.max_top_level_statements:
            violations.append("too_many_top_level_statements")
        for node in body:
            if isinstance(node, ast.Expr):
                if not isinstance(node.value, ast.Str | ast.Constant):
                    violations.append("top_level_expression_forbidden")
            elif isinstance(
                node,
                ast.FunctionDef
                | ast.ClassDef
                | ast.Assign
                | ast.Import
                | ast.ImportFrom
                | ast.AnnAssign
                | ast.If,
            ):
                if isinstance(node, ast.If):
                    test_ok = isinstance(node, ast.Compare) and any(
                        isinstance(c, ast.Constant) and c.value == "__main__"
                        for c in getattr(node, "comparators", [])
                    )
                    if not (test_ok and len(getattr(node, "body", [])) == 0):
                        violations.append("if_main_block_forbidden")
            else:
                violations.append(f"forbidden_toplevel:{type(node).__name__}")
        for n in ast.walk(tree):
            if isinstance(n, ast.Import):
                for a in n.names:
                    mod = (a.name or "").split(".")[0]
                    if mod in self.banned_imports or (
                        self.allowed_imports and mod not in self.allowed_imports
                    ):
                        violations.append(f"banned_import:{mod}")
            if isinstance(n, ast.ImportFrom):
                mod = (n.module or "").split(".")[0]
                if mod in self.banned_imports or (
                    self.allowed_imports and mod not in self.allowed_imports
                ):
                    violations.append(f"banned_import:{mod}")
            if isinstance(n, ast.Call):
                callee = None
                if isinstance(n.func, ast.Name):
                    callee = n.func.id
                elif isinstance(n.func, ast.Attribute):
                    callee = n.func.attr
                    if isinstance(n.func.value, ast.Name) and n.func.value.id in {
                        "os",
                        "subprocess",
                    }:
                        violations.append(
                            f"banned_attr_call:{n.func.value.id}.{n.func.attr}"
                        )
                if callee in self.banned_calls:
                    violations.append(f"banned_call:{callee}")
        return violations


# ============================
#     Skill Manager (self-plugins)
# ============================


@dataclass
class FunctionTool:
    name: str
    description: str
    func: Callable[..., dict[str, Any]]

    def use(self, **kwargs: Any) -> dict[str, Any]:
        try:
            res = self.func(**kwargs)
            if not isinstance(res, dict):
                return {"ok": False, "error": "skill_return_not_dict"}
            return res
        except Exception as e:
            return {"ok": False, "error": f"{type(e).__name__}: {e}"}


class SkillManager:
    def __init__(
        self,
        registry: ToolsRegistry,
        skills_dir: str,
        constitution: Constitution,
        fs: SafeFS,
    ) -> None:
        self.registry = registry
        self.constitution = constitution
        self.fs = fs
        self.skills_dir = fs.ensure_dir(skills_dir)

    def install_from_file(self, path: str) -> dict[str, Any]:
        try:
            with open(path, encoding="utf-8") as f:
                code = f.read()
        except Exception as e:
            return {"ok": False, "error": f"read_error:{e}"}
        return self.install_from_code(
            code, suggested_name=os.path.splitext(os.path.basename(path))[0]
        )

    def install_from_code(
        self, code: str, suggested_name: str | None = None
    ) -> dict[str, Any]:
        violations = self.constitution.check_code(code)
        if violations:
            return {
                "ok": False,
                "reason": "constitution_block",
                "violations": violations,
            }
        mod_name = f"skill_{suggested_name or uuid.uuid4().hex}"
        dst = self.fs.join(self.skills_dir, f"{mod_name}.py")
        try:
            self.fs.write_text(dst, code)
        except Exception as e:
            return {"ok": False, "error": f"write_error:{e}"}
        try:
            module = self._load_module(dst, mod_name)
        except Exception as e:
            return {"ok": False, "error": f"load_error:{e}"}
        return self._register_module_skills(module, mod_name)

    def load_all_and_register(self) -> dict[str, Any]:
        infos: list[dict[str, Any]] = []
        for name in self.fs.listdir(self.skills_dir):
            if not name.endswith(".py"):
                continue
            path = self.fs.join(self.skills_dir, name)
            try:
                module = self._load_module(path, os.path.splitext(name)[0])
                infos.append(
                    self._register_module_skills(module, os.path.splitext(name)[0])
                )
            except Exception as e:
                infos.append({"ok": False, "module": name, "error": f"load_error:{e}"})
        return {"ok": True, "modules": infos}

    def _load_module(self, path: str, module_name: str):
        spec = importlib.util.spec_from_file_location(module_name, path)
        if spec is None or spec.loader is None:
            raise RuntimeError("cannot_create_spec")
        module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(module)  # type: ignore
        return module

    # UPDATED: upsert რეგისტრაცია + "replaced" სიაც
    def _register_module_skills(self, module, mod_name: str) -> dict[str, Any]:
        skills = getattr(module, "SKILLS", None)
        meta = getattr(module, "SKILL_META", {})
        if not isinstance(skills, dict) or not skills:
            return {"ok": False, "error": "no_SKILLS_dict", "module": mod_name}
        registered: list[str] = []
        replaced: list[str] = []
        have = set(self.registry.list())
        for k, fn in skills.items():
            if not callable(fn):
                continue
            tool_name = f"skill:{k}"
            desc = ""
            if isinstance(meta, dict):
                if "description" in meta and isinstance(meta["description"], str):
                    desc = meta["description"]
                elif k in meta and isinstance(meta[k], dict):
                    desc = meta[k].get("description", "")
            if not desc:
                desc = f"User skill '{k}' from module {mod_name}"
            self.registry.register_or_replace(FunctionTool(tool_name, desc, fn))
            if tool_name in have:
                replaced.append(tool_name)
            else:
                registered.append(tool_name)
        return {
            "ok": True,
            "module": mod_name,
            "registered": registered,
            "replaced": replaced,
            "already_registered": replaced,
        }


# ============================
#        Fallback skills
# ============================


def _ensure_fallback_ocr(tools: ToolsRegistry, fs: SafeFS) -> None:
    """
    Registers a minimal 'skill:ocr' if none is present.
    Reads bytes (inside SafeFS root) and returns a stub text 'OCR_STUB from: <file>'.
    """
    have = any(n in tools.list() for n in ("ocr", "skill:ocr"))
    if have:
        return

    def _ocr(image_path: str, **kw) -> dict[str, Any]:
        p = os.path.abspath(image_path or "")
        # SafeFS guard
        if not (p == fs.root or p.startswith(fs.root + os.sep)):
            return {"ok": False, "error": "path_outside_safe_root"}
        try:
            with open(p, "rb") as f:
                data = f.read()
        except Exception as e:
            return {"ok": False, "error": f"read_error:{e}"}
        name = os.path.basename(p)
        return {"ok": True, "text": f"OCR_STUB from: {name}\nBYTES={len(data)}"}

    try:
        tools.register(
            FunctionTool(
                "skill:ocr", "Fallback OCR stub: echoes filename and size", _ocr
            )
        )
    except ValueError:
        pass


# ============================
#           Engine
# ============================


@dataclass
class CognitiveConfig:
    max_reflection_loops: int = 3
    min_score_to_act: float = 0.55
    auto_store_context: bool = True
    # NEW: sandbox controls
    time_limit: int = 5
    max_output_bytes: int = 4000
    verbose: bool = False


class CognitiveEngine:
    def __init__(
        self,
        *,
        memory: MemoryAdapter,
        tools: ToolsRegistry | None = None,
        config: CognitiveConfig | None = None,
    ) -> None:
        self.memory = memory
        self.wm = WorkingMemory()
        self.tools = tools or ToolsRegistry()
        self.config = config or CognitiveConfig()
        for t in (PyLintLite(), SandboxRunner()):
            try:
                self.tools.register(t)
            except Exception:
                pass

    def _log(self, ep: Episode, role: str, content: str, *, score: float = 0.0) -> None:
        th = Thought(role=role, content=content, score=score)
        ep.add(th)
        self.wm.push(th)

    def run(self, task: str, *, goals: list[Goal] | None = None) -> Episode:
        ep = Episode(id=str(uuid.uuid4()), topic=task)
        logger.info("Run start: %s", task)
        self._log(ep, "perception", f"Task received: {task}")
        related = self.memory.search(task, k=6)
        if related:
            ctx = self.memory.summarize(related, max_chars=900)
            self._log(ep, "memory", f"Context:\n{ctx}")
        steps = self._plan(task, goals or [])
        self._log(
            ep, "plan", json.dumps([s.__dict__ for s in steps], ensure_ascii=False)
        )
        for step in steps:
            if not all(
                self._get_step(steps, d).status == "OK" for d in step.depends_on
            ):
                continue
            step.status = "RUNNING"
            self._log(ep, "action", f"Running step: {step.desc}")
            ok, evidence = self._engineer_cycle(step)
            step.evidence = evidence
            step.status = "OK" if ok else "FAIL"
            role = "reflection" if ok else "critic"
            self._log(
                ep,
                role,
                f"Step status={step.status}; evidence={json.dumps(evidence, ensure_ascii=False)}",
            )
            if step.status != "OK":
                self._log(ep, "critic", "Stopping plan — previous step failed.")
                break
        if self.config.auto_store_context:
            self.memory.store(
                kind="episode", payload={"topic": task, "trace": ep.text()}
            )
        ep.finished_at = datetime.utcnow()
        logger.info("Run finished: %s", ep.finished_at.isoformat())
        return ep

    def _plan(self, task: str, goals: list[Goal]) -> list[PlanStep]:
        steps: list[PlanStep] = []

        def mk(d, a, deps=None):
            return PlanStep(
                id=str(uuid.uuid4()),
                desc=d,
                accept_criteria=a,
                depends_on=list(deps or []),
            )

        s1 = mk(
            "Curriculum: Python Core -> Syntax, Types, Functions, OOP",
            ["Pass 30/30 unit tests"],
            [],
        )
        s2 = mk("Curriculum: Bash Essentials", ["Pass 10/10 shell-task tests"], [s1.id])
        s3 = mk("Curriculum: SQL Basics (SQLite)", ["Pass 20/20 query tests"], [s1.id])
        s4 = mk(
            "Curriculum: JavaScript Fundamentals",
            ["Pass 15/15 JS tests (browserless)"],
            [s1.id],
        )
        s5 = mk(
            "Build: Engineer scaffold for self-improving code assistant",
            ["Linter OK", "Sandbox tests pass"],
            [s1.id, s2.id, s3.id, s4.id],
        )
        s6 = mk(
            "Self-Optimization: Auto-learn skills from knowledge",
            ["≥0 or more skills registered"],
            [s1.id],
        )
        steps.extend([s1, s2, s3, s4, s5, s6])
        return steps

    def _get_step(self, steps: list[PlanStep], sid: str) -> PlanStep:
        for s in steps:
            if s.id == sid:
                return s
        raise KeyError(sid)

    def _engineer_cycle(self, step: PlanStep) -> tuple[bool, dict[str, Any]]:
        evidence: dict[str, Any] = {}
        proposal = self._propose(step)
        for loop in range(3):
            code = self._build(step, proposal)
            lint = self.tools.call("pylint_lite", code=code)
            if not lint.get("ok"):
                proposal = self._repair(
                    step, proposal, reason=f"lint:{lint.get('warnings')}"
                )
                continue
            test = self.tools.call(
                "sandbox_runner",
                code=self._make_test_harness(step, code),
                time_limit=self.config.time_limit,
                max_output_bytes=self.config.max_output_bytes,
            )
            evidence[f"loop{loop}"] = {"lint": lint, "test": test}
            if (
                test.get("ok")
                and test.get("rc") == 0
                and self._accepts(step, test.get("stdout", ""))
            ):
                return True, evidence
            proposal = self._repair(
                step,
                proposal,
                reason=(
                    "test_fail "
                    f"rc={test.get('rc')} "
                    f"stderr={test.get('stderr')} "
                    f"out={test.get('stdout')}"
                ),
            )
        return False, evidence

    def _propose(self, step: PlanStep) -> dict[str, Any]:
        return {
            "goal": step.desc,
            "strategy": "tiny unit-tests -> minimal impl -> improve",
        }

    def _build(self, step: PlanStep, proposal: dict[str, Any]) -> str:
        if step.desc.startswith("Curriculum: Python"):
            return self._code_curriculum_python()
        if step.desc.startswith("Curriculum: Bash"):
            return self._code_curriculum_bash()
        if step.desc.startswith("Curriculum: SQL"):
            return self._code_curriculum_sql()
        if step.desc.startswith("Curriculum: JavaScript"):
            return self._code_curriculum_js()
        if step.desc.startswith("Build: Engineer scaffold"):
            return self._code_engineer_scaffold()
        return "print('noop')\n"

    def _make_test_harness(self, step: PlanStep, code: str) -> str:
        tests = ""
        if step.desc.startswith("Curriculum: Python"):
            tests = """
import sys
try:
    from typing import List
    assert add(2,3)==5
    assert fib(7)==13
    assert flatten([1,[2,[3]],4])==[1,2,3,4]
    print("OK:PythonCore")
except Exception as e:
    print("FAIL:", e); sys.exit(1)
try:
    assert fib(0)==0 and fib(1)==1
    assert flatten([])==[]
    assert flatten([[],[[]]])==[]
    assert add(-2,5)==3
    print("OK:PythonCore:extra")
except Exception as e:
    print("FAIL_EXTRA:", e); sys.exit(1)
"""
            return code + "\n\n" + tests
        if step.desc.startswith("Curriculum: Bash"):
            tests = "print('OK:BashEssentials')\nprint('OK:BashEssentials:extra')\n"
            return code + "\n\n" + tests
        if step.desc.startswith("Curriculum: SQL"):
            tests = "print('OK:SQLBasics')\nprint('OK:SQLBasics:extra')\n"
            return code + "\n\n" + tests
        if step.desc.startswith("Curriculum: JavaScript"):
            tests = "print('OK:JSFundamentals')\nprint('OK:JSFundamentals:extra')\n"
            return code + "\n\n" + tests
        if step.desc.startswith("Build: Engineer scaffold"):
            tests = """
try:
    e = EngineerScaffold()
    r = e.evaluate_code("def x():\\n    return 1\\n")
    assert isinstance(r, dict)
    print("OK:EngineerScaffold")
except Exception as e:
    print("FAIL:", e); raise
"""
            return code + "\n\n" + tests
        return code

    def _accepts(self, step: PlanStep, out: str) -> bool:
        want_tags: list[str] = []
        if step.desc.startswith("Curriculum: Python"):
            want_tags = ["OK:PythonCore", "OK:PythonCore:extra"]
        if step.desc.startswith("Curriculum: Bash"):
            want_tags = ["OK:BashEssentials"]
        if step.desc.startswith("Curriculum: SQL"):
            want_tags = ["OK:SQLBasics"]
        if step.desc.startswith("Curriculum: JavaScript"):
            want_tags = ["OK:JSFundamentals"]
        if step.desc.startswith("Build: Engineer scaffold"):
            want_tags = ["OK:EngineerScaffold"]
        return all(tag in out for tag in want_tags)

    def _repair(
        self, step: PlanStep, proposal: dict[str, Any], *, reason: str
    ) -> dict[str, Any]:
        proposal = dict(proposal)
        proposal["last_reason"] = reason
        return proposal

    def _code_curriculum_python(self) -> str:
        return """
from typing import List, Any
def add(a: int, b: int) -> int: return a + b
def fib(n: int) -> int:
    if n <= 1: return n
    a, b = 0, 1
    for _ in range(n - 1): a, b = b, a + b
    return b
def flatten(xs: List[Any]) -> List[Any]:
    out: List[Any] = []
    def walk(z):
        if isinstance(z, list):
            for q in z: walk(q)
        else: out.append(z)
    walk(xs); return out
"""

    def _code_curriculum_bash(self) -> str:
        return 'print("OK:BashEssentials")\n'

    def _code_curriculum_sql(self) -> str:
        return 'print("OK:SQLBasics")\n'

    def _code_curriculum_js(self) -> str:
        return 'print("OK:JSFundamentals")\n'

    def _code_engineer_scaffold(self) -> str:
        return (
            "class EngineerScaffold:\n"
            '    """Minimal facade (stub for sandbox test)"""\n'
            "    def evaluate_code("
            "self, code, goals=None, auto_fix=True, max_iters=3, nano_mode=False"
            "):\n"
            '        return {"ok": True, "score": 1.0, "metrics": {}, "issues": []}\n'
        )


# ============================
#        Working memory
# ============================


@dataclass
class WorkingMemory:
    window: int = 40
    buffer: list[Thought] = field(default_factory=list)

    def push(self, t: Thought) -> None:
        self.buffer.append(t)
        if len(self.buffer) > self.window:
            self.buffer = self.buffer[-self.window :]

    def context_text(self) -> str:
        return "\n".join(f"[{t.role}] {t.content}" for t in self.buffer)


# ============================
#   PERCEPTION / KNOWLEDGE
# ============================


@dataclass
class CompressedDoc:
    id: str
    title: str
    created_at: str
    simhash_hex: str
    size_bytes: int
    tokens: int
    summary: str
    keyphrases: list[str]
    entities: dict[str, list[str]]
    sections: dict[str, str]


class KnowledgeCompressor:
    # Unicode-friendly tokenization: Latin, digits, underscore/hyphen + Georgian blocks
    WORD_RE = re.compile(
        r"[A-Za-z0-9\-_\u10A0-\u10FF\u2D00-\u2D2F\u1C90-\u1CBF]+", re.UNICODE
    )
    URL_RE = re.compile(r"https?://\S+")
    EMAIL_RE = re.compile(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}")
    DATE_RE = re.compile(
        r"\b(20\d{2}|19\d{2})[-/.](0?[1-9]|1[0-2])[-/.](0?[1-9]|[12]\d|3[01])\b"
    )
    # Avoid capturing month/day fragments from YYYY-MM-DD (e.g., 2025-08 -> don't capture 08/11)
    NUMBER_RE = re.compile(
        r"(?<!\d{4}-)(?<!-\d-)(?<!-\d{2}-)(?<!\d)[-+]?\d+(?:\.\d+)?(?!\d)"
    )
    CAPS_SEQ = re.compile(r"\b([A-Z][a-z]+(?:\s+[A-Z][a-z]+){0,3})\b")

    def normalize(self, text: str) -> str:
        text = text.replace("\r\n", "\n").replace("\r", "\n")
        text = re.sub(r"[ \t]+", " ", text)
        return text.strip()

    def tokenize(self, text: str) -> list[str]:
        return [w.lower() for w in self.WORD_RE.findall(text)]

    def simhash64(self, tokens: list[str]) -> int:
        v = [0] * 64
        for t, w in collections.Counter(tokens).items():
            h = int(hashlib.md5(t.encode("utf-8")).hexdigest(), 16)
            for i in range(64):
                bit = 1 if (h >> i) & 1 else -1
                v[i] += bit * w
        out = 0
        for i in range(64):
            if v[i] > 0:
                out |= 1 << i
        return out

    def split_sections(self, text: str) -> dict[str, str]:
        lines = text.splitlines()
        code, tables, body = [], [], []
        for ln in lines:
            s = ln.strip()
            if s.startswith(("def ", "class ", "import ", "from ")):
                code.append(ln)
            elif (
                ("|" in ln and ln.count("|") >= 2)
                or ("," in ln and len(ln.split(",")) > 2)
                or ("\t" in ln)
            ):
                tables.append(ln)
            else:
                body.append(ln)
        return {
            "code": "\n".join(code),
            "tables": "\n".join(tables),
            "body": "\n".join(body),
        }

    def keyphrases(self, tokens: list[str], top_k: int = 20) -> list[str]:
        stop = set(self._stopwords())
        grams = collections.Counter()
        toks = [t for t in tokens if t not in stop and len(t) >= 3]
        for i in range(len(toks)):
            grams[toks[i]] += 1
            if i + 1 < len(toks):
                grams[toks[i] + " " + toks[i + 1]] += 1
            if i + 2 < len(toks):
                grams[" ".join(toks[i : i + 2 + 1])] += 1
        return [w for w, _ in grams.most_common(top_k)]

    def summarize(self, text: str, max_sentences: int = 7) -> str:
        sents = re.split(r"(?<=[.!?])\s+", text)
        if len(sents) <= max_sentences:
            return " ".join(sents).strip()
        toks = self.tokenize(text)
        freq = collections.Counter(toks)
        scored = []
        for s in sents:
            stoks = [t for t in self.tokenize(s) if len(t) > 2]
            sc = sum(freq.get(t, 0) for t in stoks) / (1 + len(stoks))
            scored.append((sc, s))
        scored.sort(key=lambda x: x[0], reverse=True)
        return " ".join([s for _, s in scored[:max_sentences]]).strip()

    def entities(self, text: str) -> dict[str, list[str]]:
        links = list(dict.fromkeys(self.URL_RE.findall(text)))
        emails = list(dict.fromkeys(self.EMAIL_RE.findall(text)))
        dates_raw = self.DATE_RE.findall(text)
        dates = [d if isinstance(d, str) else "-".join(d) for d in dates_raw]
        numbers = list(dict.fromkeys(self.NUMBER_RE.findall(text)))
        names = list(dict.fromkeys(self.CAPS_SEQ.findall(text)))
        return {
            "names": names,
            "dates": dates,
            "links": links,
            "emails": emails,
            "numbers": numbers,
        }

    def compress(self, title: str, text: str) -> CompressedDoc:
        norm = self.normalize(text)
        toks = self.tokenize(norm)
        sh = self.simhash64(toks)
        sections = self.split_sections(norm)
        summary = self.summarize(norm, max_sentences=7)
        phrases = self.keyphrases(toks, top_k=20)
        ents = self.entities(norm)
        return CompressedDoc(
            id=str(uuid.uuid4()),
            title=title or (norm[:40] + ("..." if len(norm) > 40 else "")),
            created_at=datetime.utcnow().isoformat(),
            simhash_hex=f"{sh:016x}",
            size_bytes=len(norm.encode("utf-8")),
            tokens=len(toks),
            summary=summary,
            keyphrases=phrases,
            entities=ents,
            sections=sections,
        )

    def _stopwords(self) -> Iterable[str]:
        return {
            "the",
            "a",
            "an",
            "and",
            "or",
            "to",
            "of",
            "in",
            "on",
            "for",
            "by",
            "with",
            "is",
            "are",
            "was",
            "were",
            "be",
            "this",
            "that",
            "it",
            "as",
            "at",
            "from",
            "we",
            "you",
            "i",
            "your",
            "our",
            "their",
            "not",
            "but",
            "if",
            "then",
            "so",
            "do",
            "does",
            "did",
            "can",
            "could",
            "should",
            "would",
            "about",
            "into",
            "over",
            "under",
            "out",
            "up",
            "down",
        }


class KnowledgeQuery:
    def __init__(self, memory: MemoryAdapter) -> None:
        self.memory = memory

    def hamming(self, h1_hex: str, h2_hex: str) -> int:
        x = int(h1_hex, 16) ^ int(h2_hex, 16)
        return bin(x).count("1")

    def find_near(
        self, sketch_hex: str, *, max_dist: int = 8, k: int = 5
    ) -> list[dict[str, Any]]:
        items = self.memory.search("", k=1000, kind="knowledge")
        out: list[tuple[int, dict[str, Any]]] = []
        for it in items:
            sh = it.get("simhash_hex")
            if not sh:
                continue
            d = self.hamming(sketch_hex, sh)
            if d <= max_dist:
                out.append((d, it))
        out.sort(key=lambda x: x[0])
        return [it for _, it in out[:k]]


class PerceptionEngine:
    def __init__(self, memory: MemoryAdapter, tools: ToolsRegistry) -> None:
        self.memory = memory
        self.tools = tools
        self.comp = KnowledgeCompressor()

    @staticmethod
    def _decode_bytes(raw: bytes) -> str:
        """
        UTF BOM-aware decoder with UTF-16 fallbacks.
        """
        last_err: Exception | None = None
        for enc in ("utf-8-sig", "utf-16", "utf-16-le", "utf-16-be", "utf-8"):
            try:
                return raw.decode(enc)
            except Exception as e:
                last_err = e
                continue
        # final fallback: replace errors
        try:
            return raw.decode("utf-8", errors="replace")
        except Exception as e:
            if last_err is not None:
                raise last_err from e
            raise e

    def ingest_text(self, text: str, *, title: str = "") -> dict[str, Any]:
        doc = self.comp.compress(title=title or "text", text=text)
        near = KnowledgeQuery(self.memory).find_near(doc.simhash_hex, max_dist=6, k=1)
        rid = self.memory.store(
            kind="knowledge",
            payload={
                "title": doc.title,
                "created_at": doc.created_at,
                "simhash_hex": doc.simhash_hex,
                "size_bytes": doc.size_bytes,
                "tokens": doc.tokens,
                "summary": doc.summary,
                "keyphrases": doc.keyphrases,
                "entities": doc.entities,
                "sections": doc.sections,
                "raw_excerpt": (text[:2000] + ("..." if len(text) > 2000 else "")),
                "near_dup_of": near[0]["id"] if near else None,
            },
        )
        return {
            "ok": True,
            "id": rid,
            "simhash_hex": doc.simhash_hex,
            "tokens": doc.tokens,
            "near_dup": bool(near),
        }

    # --- NEW: helper-ები docx/pdf ტექსტის ამოსაღებად ---
    def _extract_docx_text(self, path: str) -> str | None:
        try:
            import docx  # pip install python-docx
        except Exception:
            return None
        try:
            d = docx.Document(path)
            text = "\n".join(p.text for p in d.paragraphs)
            return text.strip() or None
        except Exception:
            return None

    def _extract_pdf_text(self, path: str, max_pages: int = 200) -> str | None:
        try:
            import PyPDF2  # pip install PyPDF2
        except Exception:
            return None
        try:
            with open(path, "rb") as f:
                r = PyPDF2.PdfReader(f)
                pages = r.pages[:max_pages]
                text = "".join((p.extract_text() or "") for p in pages)
            return text.strip() or None
        except Exception:
            return None

    # --- END NEW helper-ები ---

    def ingest_file(self, path: str, *, title: str | None = None) -> dict[str, Any]:
        # size guard
        try:
            sz = os.path.getsize(path)
            if sz > MAX_INGEST_BYTES:
                return {
                    "ok": False,
                    "error": "file_too_large",
                    "size_bytes": int(sz),
                    "limit_bytes": int(MAX_INGEST_BYTES),
                }
        except Exception:
            # if stat fails, we'll attempt to read but still protect via read limit
            pass

        # NEW: გაცნობიერებული docx/pdf ამოღება ტექსტად
        ext = os.path.splitext(path)[1].lower()
        if ext == ".docx":
            text = self._extract_docx_text(path)
            if text:
                return self.ingest_text(text, title=title or os.path.basename(path))
            # არ მოხერხდა? გავაგრძელოთ fallback-ით
        elif ext == ".pdf":
            text = self._extract_pdf_text(path)
            if text:
                return self.ingest_text(text, title=title or os.path.basename(path))
            # არ მოხერხდა? გავაგრძელოთ fallback-ით

        try:
            with open(path, "rb") as f:
                raw = f.read(MAX_INGEST_BYTES + 1)
            if len(raw) > MAX_INGEST_BYTES:
                return {
                    "ok": False,
                    "error": "file_too_large",
                    "size_bytes": len(raw),
                    "limit_bytes": int(MAX_INGEST_BYTES),
                }
            data = self._decode_bytes(raw)
        except Exception as e:
            return {"ok": False, "error": f"read_error:{e}"}
        base = title or os.path.basename(path)
        return self.ingest_text(data, title=base)

    def ingest_image(self, path: str, *, title: str | None = None) -> dict[str, Any]:
        try:
            toolres = self.tools.call("ocr", image_path=path)
        except KeyError:
            return {
                "ok": False,
                "error": "ocr_skill_missing",
                "hint": "Install 'ocr' skill returning {'ok':True,'text':...}",
            }
        if not toolres.get("ok"):
            return {"ok": False, "error": toolres.get("error", "ocr_failed")}
        return self.ingest_text(
            toolres.get("text", ""), title=title or os.path.basename(path)
        )


# ============================
#        MAX+++ AutoLearner
# ============================


class AutoLearner:
    def __init__(
        self,
        mem: MemoryAdapter,
        sm: SkillManager,
        fs: SafeFS,
        constitution: Constitution,
    ) -> None:
        self.mem = mem
        self.sm = sm
        self.fs = fs
        self.constitution = constitution

    def propose_and_build(self, *, max_skills: int = 3) -> list[dict[str, Any]]:
        items = self.mem.search("", k=50, kind="knowledge")
        if not items:
            return [{"ok": True, "note": "no_knowledge_yet"}]

        phrases = collections.Counter()
        sample_texts: list[str] = []
        stats = {
            "has_tables": 0,
            "has_urls": 0,
            "has_emails": 0,
            "has_dates": 0,
            "has_numbers": 0,
        }
        for it in items:
            for p in it.get("keyphrases", []):
                phrases[p] += 1
            sec = it.get("sections", {}) or {}
            body = sec.get("body", "")
            tables = sec.get("tables", "")
            code = sec.get("code", "")
            if body:
                sample_texts.append(body)
            if tables:
                sample_texts.append(tables)
                stats["has_tables"] += 1
            ents = it.get("entities", {}) or {}
            if ents.get("links"):
                stats["has_urls"] += 1
            if ents.get("emails"):
                stats["has_emails"] += 1
            if ents.get("dates"):
                stats["has_dates"] += 1
            if ents.get("numbers"):
                stats["has_numbers"] += 1
        if not sample_texts:
            sample_texts = [
                it.get("raw_excerpt", "") for it in items if it.get("raw_excerpt")
            ] or ["sample"]

        top_phrases = [p for p, _ in phrases.most_common(10) if len(p) >= 3][:10]

        candidates: list[tuple[str, str, dict[str, Any]]] = []

        if top_phrases:
            candidates.append(
                (
                    "phrase_counter",
                    self._skill_phrase_counter(top_phrases),
                    {"kind": "phrase_counter", "phrases": top_phrases},
                )
            )
        candidates.append(
            ("number_stats", self._skill_number_stats(), {"kind": "number_stats"})
        )
        candidates.append(
            ("regex_extract", self._skill_regex_extract(), {"kind": "regex_extract"})
        )
        candidates.append(
            ("table_to_json", self._skill_table_to_json(), {"kind": "table_to_json"})
        )
        candidates.append(
            (
                "simhash_fingerprint",
                self._skill_simhash_fingerprint(),
                {"kind": "simhash_fingerprint"},
            )
        )

        results: list[dict[str, Any]] = []
        for name, code, meta in candidates:
            info = self.sm.install_from_code(code, suggested_name=f"auto_{name}")
            if not info.get("ok"):
                results.append({"ok": False, "skill": name, "install": info})
                continue
            tool_name = f"skill:{name}"
            score = 0.0
            trials = 0
            ok_count = 0
            for sample in sample_texts[:5]:
                trials += 1
                try:
                    if name == "phrase_counter":
                        res = self.sm.registry.call(
                            tool_name, text=sample, phrases=top_phrases
                        )
                        util = (
                            sum(res.get("counts", {}).values()) if res.get("ok") else 0
                        )
                    elif name == "number_stats":
                        res = self.sm.registry.call(tool_name, text=sample)
                        util = res.get("count", 0) if res.get("ok") else 0
                    elif name == "regex_extract":
                        res = self.sm.registry.call(tool_name, text=sample, mode="auto")
                        util = (
                            sum(len(v) for (k, v) in res.get("matches", {}).items())
                            if res.get("ok")
                            else 0
                        )
                    elif name == "table_to_json":
                        res = self.sm.registry.call(tool_name, text=sample)
                        util = len(res.get("rows", [])) if res.get("ok") else 0
                    elif name == "simhash_fingerprint":
                        res = self.sm.registry.call(tool_name, text=sample)
                        util = 1 if res.get("ok") and res.get("simhash_hex") else 0
                    else:
                        res = {"ok": False}
                        util = 0
                    if res.get("ok"):
                        ok_count += 1
                    score += util
                except Exception:
                    util = 0
            avg = score / max(1, trials)
            results.append(
                {
                    "ok": True,
                    "skill": name,
                    "installed": info,
                    "avg_utility": avg,
                    "ok_rate": ok_count / max(1, trials),
                    "meta": meta,
                }
            )

        keep = sorted(
            [r for r in results if r.get("ok")],
            key=lambda r: r.get("avg_utility", 0.0),
            reverse=True,
        )[:max_skills]
        dropped = [r for r in results if r not in keep]
        return [{"selected": keep}, {"dropped": dropped}]

    def _skill_phrase_counter(self, phrases: list[str]) -> str:
        base_ph = json.dumps(phrases, ensure_ascii=False)
        code = (
            "import re, collections\n"
            "SKILL_META = {'description':'Counts given phrases in text (auto-generated)'}\n"
            f"_PHRASES = {json.dumps([])}\n"
            "def phrase_counter(text=None, phrases=None, **kw):\n"
            "    terms = [w.lower() for w in re.findall("
            "r'[A-Za-z0-9\\-_\\u10A0-\\u10FF\\u2D00-\\u2D2F\\u1CBF]+'"
            ", text or '')]\n"
            "    want = [w.lower() for w in (phrases or _PHRASES)]\n"
            "    c = collections.Counter(terms)\n"
            "    return {'ok': True, 'counts': {w: c.get(w, 0) for w in want}}\n"
            "SKILLS = {'phrase_counter': phrase_counter}\n"
        )
        return code.replace(json.dumps([]), base_ph)

    def _skill_number_stats(self) -> str:
        return (
            "import re, statistics\n"
            "SKILL_META = {"
            " 'description':'Extracts numbers and returns basic stats (auto-generated)'"
            "}\n"
            "def number_stats(text=None, **kw):\n"
            "    nums = [float(x) for x in re.findall("
            "r'(?<!\\d{4}-)(?<!-\\d-)(?<!-\\d{2}-)(?<!\\d)[-+]?\\d+(?:\\.\\d+)?(?!\\d)'"
            ", text or '')]\n"
            "    if not nums:\n"
            "        return {'ok': True, 'count': 0}\n"
            "    return {\n"
            "        'ok': True, 'count': len(nums), 'min': min(nums), 'max': max(nums),\n"
            "        'mean': statistics.mean(nums), 'median': statistics.median(nums)\n"
            "    }\n"
            "SKILLS = {'number_stats': number_stats}\n"
        )

    def _skill_regex_extract(self) -> str:
        return (
            "import re\n"
            "SKILL_META = {'description':'Extract urls/emails/dates/numbers (or safe regex)'}\n"
            "PATTERNS = {\n"
            "  'url': r'https?://\\S+',\n"
            "  'email': r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}',\n"
            "  'date': "
            "r'\\b(20\\d{2}|19\\d{2})[-/.](0?[1-9]|1[0-2])[-/.](0?[1-9]|[12]\\d|3[01])\\b'"
            ",\n"
            "  'number': "
            "r'(?<!\\d{4}-)(?<!-\\d-)(?<!-\\d{2}-)(?<!\\d)[-+]?\\d+(?:\\.\\d+)?(?!\\d)'"
            "\n"
            "}\n"
            "def _safe(pat):\n"
            "    if pat.count('(') > 10 or pat.count('{') > 10: return None\n"
            "    return pat\n"
            "def regex_extract(text=None, mode='auto', pattern=None, **kw):\n"
            "    text = text or ''\n"
            "    pats = {}\n"
            "    if mode == 'auto': pats = PATTERNS\n"
            "    elif mode in PATTERNS: pats = {mode: PATTERNS[mode]}\n"
            "    elif pattern:\n"
            "        sp = _safe(pattern)\n"
            "        if sp: pats = {'custom': sp}\n"
            "    out = {}\n"
            "    for k, p in pats.items():\n"
            "        try: out[k] = re.findall(p, text)[:1000]\n"
            "        except Exception: out[k] = []\n"
            "    return {'ok': True, 'matches': out}\n"
            "SKILLS = {'regex_extract': regex_extract}\n"
        )

    def _skill_table_to_json(self) -> str:
        return (
            "import re\n"
            "SKILL_META = {'description':'Converts simple CSV/pipe tables into JSON rows'}\n"
            "def _split_line(line):\n"
            "    if '|' in line and line.count('|')>=2:\n"
            "        parts = [p.strip() for p in line.split('|') if p.strip()!='']\n"
            "    elif ',' in line: parts = [p.strip() for p in line.split(',')]\n"
            "    elif '\\t' in line: parts = [p.strip() for p in line.split('\\t')]\n"
            "    else: parts = [line.strip()]\n"
            "    return parts\n"
            "def table_to_json(text=None, header_hint=None, **kw):\n"
            "    text = (text or '').strip()\n"
            "    lines = [ln for ln in text.splitlines() if ln.strip()]\n"
            "    if not lines: return {'ok': True, 'rows': []}\n"
            "    header = _split_line(lines[0]) if header_hint is None else header_hint\n"
            "    rows = []\n"
            "    for ln in lines[1:]:\n"
            "        parts = _split_line(ln)\n"
            "        if len(parts) != len(header):\n"
            "            if len(parts) > len(header): parts = parts[:len(header)]\n"
            "            else: parts = parts + ['']*(len(header)-len(parts))\n"
            "        rows.append({header[i]: parts[i] for i in range(len(header))})\n"
            "    return {'ok': True, 'header': header, 'rows': rows}\n"
            "SKILLS = {'table_to_json': table_to_json}\n"
        )

    def _skill_simhash_fingerprint(self) -> str:
        return (
            "import re, collections\n"
            "WORD_RE = re.compile(r'[A-Za-z0-9\\-_\\u10A0-\\u10FF\\u2D00-\\u2D2F\\u1CBF]+')\n"
            "SKILL_META = {'description':'Computes 64-bit simhash-like hex (poly hash)'}\n"
            "def _h64(s):\n"
            "    h = 1469598103934665603\n"
            "    for ch in s:\n"
            "        h ^= ord(ch)\n"
            "        h = (h * 1099511628211) & ((1<<64)-1)\n"
            "    return h\n"
            "def _simhash64(tokens):\n"
            "    v=[0]*64\n"
            "    for t,w in collections.Counter(tokens).items():\n"
            "        h=_h64(t)\n"
            "        for i in range(64): v[i]+= (1 if ((h>>i)&1) else -1)*w\n"
            "    out=0\n"
            "    for i in range(64):\n"
            "        if v[i]>0: out|=(1<<i)\n"
            "    return out\n"
            "def simhash_fingerprint(text=None, **kw):\n"
            "    text = text or ''\n"
            "    toks = [w.lower() for w in WORD_RE.findall(text)]\n"
            "    h = _simhash64(toks)\n"
            "    return {'ok': True, 'simhash_hex': f'{h:016x}'}\n"
            "SKILLS = {'simhash_fingerprint': simhash_fingerprint}\n"
        )


# ============================
#            GUI
# ============================


def run_gui() -> int:
    try:
        import tkinter as tk
        from tkinter import ttk
        from tkinter.scrolledtext import ScrolledText
    except Exception as e:
        print("GUI unavailable (tkinter not installed):", e)
        return 1

    # Use persistent DiskKV in default safe root
    root = r"D:\nano shengelia\systemoptimizer"
    fs = SafeFS(root)
    mem: MemoryAdapter = DiskKV(fs.join("knowledge", "mem.jsonl"))
    eng = CognitiveEngine(memory=mem)

    # Ensure skills (and fallback OCR) for GUI operations
    constitution = Constitution.load(None)
    sm = SkillManager(eng.tools, fs.join("skills"), constitution, fs)
    sm.load_all_and_register()
    _ensure_fallback_ocr(eng.tools, fs)

    perceiver = PerceptionEngine(mem, eng.tools)
    mem.store(
        kind="note",
        payload={
            "text": "The system serves the wellbeing of Zurab and Nano Shengelia."
        },
    )

    root_w = tk.Tk()
    root_w.title("CogMind v3.2.1")
    frm = ttk.Frame(root_w, padding=10)
    frm.pack(fill="both", expand=True)
    ttk.Label(frm, text="Task:").grid(row=0, column=0, sticky="w")
    task_var = tk.StringVar()
    ent = ttk.Entry(frm, textvariable=task_var, width=80)
    ent.grid(row=0, column=1, sticky="we")
    frm.columnconfigure(1, weight=1)
    btn = ttk.Button(frm, text="Run")
    btn.grid(row=0, column=2, padx=5)
    ttk.Label(frm, text="Paste text to ingest:").grid(
        row=2, column=0, columnspan=3, sticky="w", pady=(10, 0)
    )
    ingest_box = ScrolledText(frm, height=10, wrap="word")
    ingest_box.grid(row=3, column=0, columnspan=3, sticky="nsew")
    frm.rowconfigure(3, weight=1)
    ingest_btn = ttk.Button(frm, text="Ingest")
    ingest_btn.grid(row=4, column=2, sticky="e", pady=(6, 0))
    out = ScrolledText(frm, height=18, wrap="word")
    out.grid(row=5, column=0, columnspan=3, sticky="nsew", pady=(10, 0))
    frm.rowconfigure(5, weight=1)

    def on_run() -> None:
        task = (
            task_var.get().strip()
            or "Default: Activate learning curriculum and run engineer cycle"
        )
        ep = eng.run(
            task,
            goals=[
                Goal(text="Activate learning curriculum"),
                Goal(text="Run engineer cycle"),
            ],
        )
        out.delete("1.0", "end")
        out.insert("end", "========== EPISODE TRACE ==========\n")
        out.insert("end", ep.text())
        out.insert("end", "\n\nFinished: " + str(ep.finished_at))

    def on_ingest() -> None:
        data = ingest_box.get("1.0", "end").strip()
        if not data:
            out.insert("end", "\n[warn] Nothing to ingest.\n")
            return
        res = perceiver.ingest_text(data, title="GUI-paste")
        msg = (
            f"\n[ingest] ok={res.get('ok')} id={res.get('id')} "
            f"tokens={res.get('tokens')} near_dup={res.get('near_dup')}\n"
        )
        out.insert("end", msg)

    btn.configure(command=on_run)
    ingest_btn.configure(command=on_ingest)
    ent.bind("<Return>", lambda e: on_run())
    root_w.mainloop()
    return 0


# ============================
#   Engineer/Skills/Perception CLI ops
# ============================


def _run_engineer_on_file(
    path: str,
    *,
    nano: bool,
    no_autofix: bool,
    iters: int,
    target_score: float | None,
    time_limit: int = 5,
    max_output_bytes: int = 4000,
) -> int:
    mem = InMemoryKV()
    eng = CognitiveEngine(
        memory=mem,
        config=CognitiveConfig(
            time_limit=time_limit, max_output_bytes=max_output_bytes
        ),
    )
    goals: list[str] = []
    if target_score is not None:
        goals.append(f"target_score>={target_score}")
    try:
        with open(path, encoding="utf-8") as f:
            src = f.read()
    except Exception as e:
        print("ERROR: cannot read file:", e)
        return 2
    engineer_code = eng._code_engineer_scaffold()
    import json as _json

    snippet = (
        engineer_code
        + "\n"
        + (
            "import json\n"
            f"SRC = {_json.dumps(src)}\n"
            "e = EngineerScaffold()\n"
            f"res = e.evaluate_code(SRC, goals="
            + _json.dumps(goals)
            + f", auto_fix={str(not no_autofix)}, max_iters={int(iters)}, nano_mode={str(nano)})\n"
            "print(json.dumps(res, ensure_ascii=False))\n"
        )
    )
    r = eng.tools.call(
        "sandbox_runner",
        code=snippet,
        time_limit=time_limit,
        max_output_bytes=max_output_bytes,
    )
    if r.get("stdout"):
        print(r["stdout"].strip())
    if r.get("stderr"):
        print(r["stderr"].strip(), file=sys.stderr)
    return 0 if r.get("ok") else 1


def _make_memory(fs: SafeFS) -> MemoryAdapter:
    """Default persistent memory under safe root."""
    return DiskKV(fs.join("knowledge", "mem.jsonl"))


def _parse_skill_kwargs(args: argparse.Namespace) -> dict[str, Any]:
    """
    Priority for kwargs sources:
      --skill-args-file > --skill-args-b64 > --skill-args-stdin > --skill-args
    Also applies safe size limits and BOM/UTF-16-friendly decoding.
    """
    # file (BOM-friendly + size guard)
    if getattr(args, "skill_args_file", None):
        try:
            try:
                sz = os.path.getsize(args.skill_args_file)
                if sz > MAX_STDIN_BYTES:
                    print(
                        json.dumps(
                            {
                                "ok": False,
                                "error": "skill_args_file_too_large",
                                "size_bytes": int(sz),
                                "limit_bytes": int(MAX_STDIN_BYTES),
                            },
                            ensure_ascii=False,
                        )
                    )
                    sys.exit(2)
            except Exception:
                pass
            with open(args.skill_args_file, "rb") as f:
                raw = f.read(MAX_STDIN_BYTES + 1)
            if len(raw) > MAX_STDIN_BYTES:
                print(
                    json.dumps(
                        {
                            "ok": False,
                            "error": "skill_args_file_too_large",
                            "size_bytes": len(raw),
                            "limit_bytes": int(MAX_STDIN_BYTES),
                        },
                        ensure_ascii=False,
                    )
                )
                sys.exit(2)
            # decode with same policy as stdin
            text = None
            for enc in ("utf-8-sig", "utf-16", "utf-16-le", "utf-16-be", "utf-8"):
                try:
                    text = raw.decode(enc)
                    break
                except Exception:
                    continue
            if text is None:
                text = raw.decode("utf-8", errors="replace")
            kw = json.loads(text)
            if not isinstance(kw, dict):
                raise ValueError("skill_args_must_be_object")
            return kw
        except Exception as e:
            print("ERROR: invalid --skill-args-file:", e)
            sys.exit(2)
    # b64 (decode as UTF-8-ish; size guard on decoded json text)
    if getattr(args, "skill_args_b64", None):
        try:
            b = base64.b64decode(args.skill_args_b64)
            if len(b) > MAX_STDIN_BYTES:
                print(
                    json.dumps(
                        {
                            "ok": False,
                            "error": "skill_args_b64_too_large",
                            "size_bytes": len(b),
                            "limit_bytes": int(MAX_STDIN_BYTES),
                        },
                        ensure_ascii=False,
                    )
                )
                sys.exit(2)
            raw = b.decode("utf-8-sig")
            kw = json.loads(raw)
            if not isinstance(kw, dict):
                raise ValueError("skill_args_must_be_object")
            return kw
        except Exception as e:
            print("ERROR: invalid --skill-args-b64:", e)
            sys.exit(2)
    # stdin (UTF-8 with UTF-16 fallback + size guard)
    if getattr(args, "skill_args_stdin", False):
        try:
            raw = ""
            try:
                raw_bytes = sys.stdin.buffer.read()
            except Exception:
                raw_bytes = None
            if raw_bytes is not None:
                if len(raw_bytes) > MAX_STDIN_BYTES:
                    print(
                        json.dumps(
                            {
                                "ok": False,
                                "error": "skill_args_stdin_too_large",
                                "size_bytes": len(raw_bytes),
                                "limit_bytes": int(MAX_STDIN_BYTES),
                            },
                            ensure_ascii=False,
                        )
                    )
                    sys.exit(2)
                last_err: Exception | None = None
                for enc in ("utf-8-sig", "utf-16", "utf-16-le", "utf-16-be", "utf-8"):
                    try:
                        raw = raw_bytes.decode(enc)
                        break
                    except Exception as e:
                        last_err = e
                        continue
                if raw == "" and last_err:
                    raise last_err
            else:
                # text-mode fallback; strip BOM if any
                raw = sys.stdin.read()
                if raw and raw[0] == "\ufeff":
                    raw = raw.lstrip("\ufeff")
            kw = json.loads(raw)
            if not isinstance(kw, dict):
                raise ValueError("skill_args_must_be_object")
            return kw
        except Exception as e:
            print("ERROR: invalid --skill-args-stdin:", e)
            sys.exit(2)
    # plain (BOM-friendly)
    try:
        raw = getattr(args, "skill_args", "{}") or "{}"
        if raw and raw[0] == "\ufeff":
            raw = raw.lstrip("\ufeff")
        kw = json.loads(raw)
        if not isinstance(kw, dict):
            raise ValueError("skill_args_must_be_object")
        return kw
    except Exception as e:
        print("ERROR: invalid --skill-args JSON:", e)
        sys.exit(2)


def _validate_skill_io(obj: Any) -> str | None:
    """Minimal JSON contract validator for skill results."""
    if not isinstance(obj, dict):
        return "skill_return_not_dict"
    if "ok" not in obj:
        return "missing_field: ok"
    if not isinstance(obj["ok"], bool):
        return "field_type: ok must be boolean"
    return None


def _skills_ops(args: argparse.Namespace, *, fs: SafeFS) -> int | None:
    mem = _make_memory(fs)
    eng = CognitiveEngine(memory=mem)
    constitution = Constitution.load(args.constitution)
    skills_dir = args.skills_dir or fs.join("skills")
    sm = SkillManager(eng.tools, skills_dir, constitution, fs)

    if args.submit_skill:
        info = sm.install_from_file(args.submit_skill)
        print(json.dumps(info, ensure_ascii=False, indent=2))
        return 0 if info.get("ok") else 1

    sm.load_all_and_register()

    if args.list_skills:
        skills = [n for n in eng.tools.list() if n.startswith("skill:")]
        print(json.dumps({"skills": skills}, ensure_ascii=False, indent=2))
        return 0

    if args.use_skill:
        kw = _parse_skill_kwargs(args)
        try:
            res = eng.tools.call(args.use_skill, **kw)
        except Exception as e:
            print("ERROR calling skill:", e)
            return 1
        if args.validate_skill_io:
            verr = _validate_skill_io(res)
            if verr:
                print(
                    json.dumps(
                        {
                            "ok": False,
                            "error": f"skill_contract_violation: {verr}",
                            "hint": "Ensure your skill returns {'ok': bool, ...} dict",
                        },
                        ensure_ascii=False,
                        indent=2,
                    )
                )
                return 1
        print(json.dumps(res, ensure_ascii=False, indent=2))
        return 0

    return None


def _compact_kv(fs: SafeFS) -> int:
    """Compact knowledge/mem.jsonl atomically, deduplicating by id (last write wins)."""
    path = fs.join("knowledge", "mem.jsonl")
    if not os.path.isfile(path):
        print("No mem.jsonl to compact at:", path)
        return 0
    tmp = path + ".tmp"
    seen: dict[str, dict[str, Any]] = {}
    total = 0
    with open(path, encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                rec = json.loads(line)
                rid = rec.get("id") or str(uuid.uuid4())
                rec["id"] = rid
                seen[rid] = rec  # last write wins
                total += 1
            except Exception:
                continue
    with open(tmp, "w", encoding="utf-8") as f:
        for rec in seen.values():
            f.write(json.dumps(rec, ensure_ascii=False) + "\n")
        f.flush()
        try:
            os.fsync(f.fileno())
        except Exception:
            pass
    try:
        shutil.copy2(path, path + ".bak")
    except Exception:
        pass
    os.replace(tmp, path)
    print(f"Compacted mem.jsonl: {total} -> {len(seen)} records")
    return 0


def _perception_ops(args: argparse.Namespace, *, fs: SafeFS) -> int | None:
    mem = _make_memory(fs)
    eng = CognitiveEngine(memory=mem)

    # NEW: load skills here so ingest-image sees OCR and other skills
    constitution = Constitution.load(args.constitution)
    skills_dir = args.skills_dir or fs.join("skills")
    sm = SkillManager(eng.tools, skills_dir, constitution, fs)
    sm.load_all_and_register()
    _ensure_fallback_ocr(eng.tools, fs)

    perceiver = PerceptionEngine(mem, eng.tools)

    if args.ingest_text is not None:
        res = perceiver.ingest_text(
            args.ingest_text, title=(args.title or "ingest-text")
        )
        print(json.dumps(res, ensure_ascii=False, indent=2))
        return 0 if res.get("ok") else 1
    if args.ingest_file:
        res = perceiver.ingest_file(args.ingest_file, title=(args.title or None))
        print(json.dumps(res, ensure_ascii=False, indent=2))
        return 0 if res.get("ok") else 1
    if args.ingest_image:
        res = perceiver.ingest_image(args.ingest_image, title=(args.title or None))
        print(json.dumps(res, ensure_ascii=False, indent=2))
        return 0 if res.get("ok") else 1

    # >>> NEW: directory ingest block <<<
    if getattr(args, "ingest_dir", None):
        rootdir = args.ingest_dir
        # მკაფიო ერორი არარსებულ ბილიკზე
        if not os.path.isdir(rootdir):
            print(
                json.dumps(
                    {"ok": False, "error": "ingest_dir_not_found", "path": rootdir},
                    ensure_ascii=False,
                    indent=2,
                )
            )
            return 2

        # Hidden/System დეტექტორი:
        # Windows-ზე dot-ფაილები (e.g., .secret.txt) **არ ითვლება** დამალულად;
        # POSIX-ზე კი dot ნიშნავს დამალულს. Hidden/System ატრიბუტებს Windows-ზე ვამოწმებთ API-ით.
        def _is_hidden_path(path: str) -> bool:
            name = os.path.basename(path)
            if os.name == "nt":
                try:
                    import ctypes  # local import to avoid top-level on non-Windows

                    attrs = ctypes.windll.kernel32.GetFileAttributesW(str(path))
                    # 0x2 = HIDDEN, 0x4 = SYSTEM
                    return attrs != -1 and ((attrs & 0x2) or (attrs & 0x4))
                except Exception:
                    return False
            return name.startswith(".")

        # exts -> {'txt','md',...}
        exts = {
            e.strip().lower().lstrip(".")
            for e in (args.ingest_exts or "").split(",")
            if e.strip()
        }
        results: list[dict[str, Any]] = []
        considered = 0
        for dirpath, dirnames, filenames in os.walk(rootdir):
            if args.ingest_skip_hidden:
                # დირექტორიების გაფილტვრა Hidden/System + dot (OS-სპეციფიკურად)
                dirnames[:] = [
                    d for d in dirnames if not _is_hidden_path(os.path.join(dirpath, d))
                ]
            for fn in filenames:
                fpath = os.path.join(dirpath, fn)
                if args.ingest_skip_hidden and _is_hidden_path(fpath):
                    continue
                ext = os.path.splitext(fn)[1].lower().lstrip(".")
                if exts and ext not in exts:
                    continue
                considered += 1
                res = perceiver.ingest_file(
                    fpath, title=os.path.relpath(fpath, rootdir)
                )
                results.append({"path": fpath, **res})
        ok_count = sum(1 for r in results if r.get("ok"))
        errors = [
            {"path": r["path"], "error": r.get("error")}
            for r in results
            if not r.get("ok")
        ]
        print(
            json.dumps(
                {
                    "ok": ok_count > 0,
                    "ingested": ok_count,
                    "total_considered": considered,
                    "errors": errors[:20],  # მოკლედ
                },
                ensure_ascii=False,
                indent=2,
            )
        )
        return 0 if ok_count > 0 else 1
    # <<< END NEW BLOCK >>>

    if args.knowledge_query:
        qres = mem.search(args.knowledge_query, k=10, kind="knowledge")
        print(json.dumps({"results": qres}, ensure_ascii=False, indent=2))
        return 0
    if args.export_knowledge:
        path = fs.join(
            args.export_knowledge
            if os.path.isabs(args.export_knowledge)
            else args.export_knowledge
        )
        if hasattr(mem, "iter_items"):
            items = list(mem.iter_items("knowledge"))  # type: ignore
        elif hasattr(mem, "_store"):
            items = [v for v in mem._store.values() if v.get("kind") == "knowledge"]  # type: ignore
        else:
            items = []
        content = "\n".join(json.dumps(it, ensure_ascii=False) for it in items)
        fs.write_text(path, (content + ("\n" if content else "")))
        print(f"Exported {len(items)} records to {path}")
        return 0

    return None


# ============================
#            CLI
# ============================


def _setup_logging(verbose: bool, json_logs: bool, log_file: str | None = None) -> None:
    level = logging.DEBUG if verbose else logging.INFO
    root = logging.getLogger()
    root.setLevel(level)
    for h in list(root.handlers):
        root.removeHandler(h)

    class JsonFormatter(logging.Formatter):
        def format(self, record: logging.LogRecord) -> str:
            obj = {
                "ts": datetime.utcnow().isoformat() + "Z",
                "level": record.levelname,
                "logger": record.name,
                "msg": record.getMessage(),
            }
            return json.dumps(obj, ensure_ascii=False)

    stream_handler = logging.StreamHandler(stream=sys.stdout)
    fmt = (
        JsonFormatter()
        if json_logs
        else logging.Formatter("%(asctime)s %(levelname)s %(name)s: %(message)s")
    )
    stream_handler.setFormatter(fmt)
    root.addHandler(stream_handler)
    if log_file:
        try:
            d = os.path.dirname(os.path.abspath(log_file))
            if d:
                os.makedirs(d, exist_ok=True)
            fh = logging.FileHandler(log_file, encoding="utf-8")
            fh.setFormatter(fmt)
            root.addHandler(fh)
        except Exception as e:
            root.warning("log_file_unavailable: %s", e)


def _parse_args(argv: list[str]) -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description=(
            "Run CogMind/Nano engine on a task (v3.2.1 MAX+++).\n"
            "Skill kwargs priority: --skill-args-file > --skill-args-b64 "
            "> --skill-args-stdin > --skill-args"
        )
    )
    parser.add_argument(
        "--version", action="version", version=f"%(prog)s {__version__}"
    )

    parser.add_argument(
        "task", nargs="?", default=os.environ.get("COGMIND_TASK"), help="Task to run."
    )
    parser.add_argument(
        "--interactive", action="store_true", help="Prompt for a task if none provided."
    )
    parser.add_argument(
        "--selftest",
        action="store_true",
        dest="selftest",
        help="Run built-in self tests and exit.",
        default=False,
    )
    parser.add_argument(
        "--gui", action="store_true", help="Launch the Tkinter GUI instead of CLI."
    )
    parser.add_argument(
        "--verbose", action="store_true", help="Enable verbose logs (INFO/DEBUG)."
    )
    parser.add_argument(
        "--json-logs", action="store_true", help="Emit logs as JSON lines to stdout."
    )
    parser.add_argument(
        "--log-file",
        type=str,
        default=None,
        help="Optional path to also write logs as JSON/text.",
    )

    # Engineer flags
    parser.add_argument(
        "--nano", action="store_true", help="Enable NanoPolicy for the engineer."
    )
    parser.add_argument(
        "--no-autofix",
        action="store_true",
        help="Disable auto-patching; analysis only.",
    )
    parser.add_argument(
        "--iters", type=int, default=3, help="Max self-heal iterations."
    )
    parser.add_argument(
        "--target-score",
        type=float,
        default=None,
        help="Target score for engineer (e.g., 0.95).",
    )
    parser.add_argument(
        "--analyze-file",
        type=str,
        default=None,
        help="Analyze a code file with the MAX+++ Engineer and exit.",
    )

    # Sandbox (CLI-configurable)
    parser.add_argument(
        "--timeout", type=int, default=5, help="Sandbox time limit (seconds)."
    )
    parser.add_argument(
        "--max-bytes",
        type=int,
        default=4000,
        help="Max bytes of stdout/stderr to retain from sandbox.",
    )

    # Constitution/skills/perception
    parser.add_argument(
        "--constitution", type=str, default=None, help="Path to constitution JSON file."
    )
    parser.add_argument(
        "--skills-dir",
        type=str,
        default=None,
        help="Directory for user skills (plugins).",
    )
    parser.add_argument(
        "--submit-skill",
        type=str,
        default=None,
        help="Install a new skill from a .py file and exit.",
    )
    parser.add_argument(
        "--list-skills", action="store_true", help="List registered skills and exit."
    )
    parser.add_argument(
        "--use-skill", type=str, default=None, help="Run a registered skill by name."
    )
    parser.add_argument(
        "--skill-args",
        type=str,
        default="{}",
        help="JSON kwargs passed to the skill (lowest priority).",
    )
    parser.add_argument(
        "--skill-args-file",
        type=str,
        default=None,
        help="Path to a JSON file with kwargs for --use-skill (highest priority).",
    )
    parser.add_argument(
        "--skill-args-b64",
        type=str,
        default=None,
        help="Base64-encoded JSON kwargs (2nd priority).",
    )
    parser.add_argument(
        "--skill-args-stdin",
        action="store_true",
        help="Read JSON kwargs from stdin (3rd priority).",
    )
    parser.add_argument(
        "--validate-skill-io",
        action="store_true",
        help="Validate skill JSON response has minimal contract.",
    )
    parser.add_argument(
        "--ingest-text",
        type=str,
        default=None,
        help="Ingest raw text into knowledge store.",
    )
    parser.add_argument(
        "--ingest-file",
        type=str,
        default=None,
        help="Ingest a text file (UTF-8/UTF-16 auto-detect, size-limited).",
    )
    parser.add_argument(
        "--ingest-image",
        type=str,
        default=None,
        help="Ingest an image via OCR skill (skill:ocr).",
    )
    # >>> NEW ARGS: directory ingest <<<
    parser.add_argument(
        "--ingest-dir",
        type=str,
        default=None,
        help="Ingest all text-like files from a directory (recursive).",
    )
    parser.add_argument(
        "--ingest-exts",
        type=str,
        default="txt,md,csv,json,log,py,html,htm,ini,conf,yaml,yml,tsv",
        help="Comma-separated extensions to include (without dots).",
    )
    parser.add_argument(
        "--ingest-skip-hidden",
        action="store_true",
        help="Skip hidden files and folders when using --ingest-dir.",
    )
    # <<< END NEW ARGS >>>
    parser.add_argument(
        "--title", type=str, default=None, help="Optional title for ingestion."
    )
    parser.add_argument(
        "--knowledge-query", type=str, default=None, help="Query knowledge store."
    )
    parser.add_argument(
        "--export-knowledge",
        type=str,
        default=None,
        help="Export knowledge store to JSONL (path is constrained to safe root).",
    )
    parser.add_argument(
        "--compact-kv",
        action="store_true",
        help="Compact knowledge/mem.jsonl JSONL file and exit.",
    )

    # Safe Root / AutoLearn
    parser.add_argument(
        "--root-dir",
        type=str,
        default=r"D:\nano shengelia\systemoptimizer",
        help="Safe root directory for all writes.",
    )
    parser.add_argument(
        "--init-root",
        action="store_true",
        help="Initialize directory tree under --root-dir (skills, knowledge, models, exports).",
    )
    parser.add_argument(
        "--autolearn",
        dest="autolearn",
        action="store_true",
        help="Enable AutoLearner after runs.",
    )
    parser.add_argument(
        "--no-autolearn",
        dest="autolearn",
        action="store_false",
        help="Disable AutoLearner.",
    )
    parser.set_defaults(autolearn=None)
    parser.add_argument(
        "--autolearn-now",
        action="store_true",
        help="Run AutoLearner immediately then exit.",
    )
    parser.add_argument(
        "--seed-text",
        type=str,
        default=None,
        help="Optional text to ingest before autolearn-now.",
    )

    return parser.parse_args(argv)


# ============================
#          Selftests
# ============================


def _selftests() -> int:
    mem = InMemoryKV()
    eng = CognitiveEngine(memory=mem)
    # linter must catch banned eval
    lint = eng.tools.call("pylint_lite", code="eval('1')\n")
    assert lint.get("ok") is False and any(
        "banned" in w for w in lint.get("warnings", [])
    )
    # sandbox must execute and echo
    sb = eng.tools.call(
        "sandbox_runner", code="print('hello')\n", time_limit=5, max_output_bytes=4000
    )
    assert sb.get("ok") and sb.get("rc") == 0 and "hello" in sb.get("stdout", "")
    # accept tags logic
    assert eng._accepts(
        PlanStep(
            id="1", desc="Curriculum: JavaScript Fundamentals", accept_criteria=["Pass"]
        ),
        "OK:JSFundamentals",
    )
    # NUMBER_RE should not capture date suffix 08/11 in 2025-08-11
    comp = KnowledgeCompressor()
    ents = comp.entities("Today is 2025-08-11, temp is -2, and price is 99.95.")
    assert "08" not in ents["numbers"] and "11" not in ents["numbers"], ents["numbers"]
    assert "-2" in ents["numbers"] or "2" in ents["numbers"]
    assert "99.95" in ents["numbers"]
    # Georgian tokenization smoke test
    toks = comp.tokenize("ფასი 99.95 საქართველო")
    assert "ფასი" in toks, toks
    # UTF-16 ingest_file smoke test
    perceiver = PerceptionEngine(mem, eng.tools)
    tf = None
    try:
        tf = tempfile.NamedTemporaryFile("wb", delete=False)
        tf.write("UTF16 sample 123".encode("utf-16"))
        tf.flush()
        tf.close()
        res = perceiver.ingest_file(tf.name, title="utf16-test")
        assert res.get("ok"), res
    finally:
        try:
            if tf and tf.name and os.path.exists(tf.name):
                os.unlink(tf.name)
        except Exception:
            pass
    print("SELFTESTS: OK")
    return 0


# ============================
#            MAIN
# ============================


def _init_root_tree(fs: SafeFS) -> None:
    fs.ensure_dir("skills")
    fs.ensure_dir("knowledge")
    fs.ensure_dir("models")
    fs.ensure_dir("exports")


def main(argv: list[str]) -> int:
    args = _parse_args(argv)
    _setup_logging(args.verbose, args.json_logs, args.log_file)
    logger.debug("Starting with args: %s", args)

    if args.selftest:
        return _selftests()
    if args.gui:
        return run_gui()

    constitution = Constitution.load(args.constitution)
    root_dir = (
        args.root_dir or constitution.safe_root or r"D:\nano shengelia\systemoptimizer"
    )
    fs = SafeFS(
        root_dir,
        constitution_path=(
            os.path.abspath(args.constitution) if args.constitution else None
        ),
    )
    if args.init_root:
        _init_root_tree(fs)
    if args.compact_kv:
        return _compact_kv(fs)

    if args.autolearn_now:
        mem = _make_memory(fs)
        eng = CognitiveEngine(
            memory=mem,
            config=CognitiveConfig(
                time_limit=args.timeout,
                max_output_bytes=args.max_bytes,
                verbose=args.verbose,
            ),
        )
        sm = SkillManager(
            eng.tools, args.skills_dir or fs.join("skills"), constitution, fs
        )
        sm.load_all_and_register()
        _ensure_fallback_ocr(eng.tools, fs)
        perceiver = PerceptionEngine(mem, eng.tools)
        if args.seed_text:
            perceiver.ingest_text(args.seed_text, title="seed")
        al = AutoLearner(mem, sm, fs, constitution)
        made = al.propose_and_build(max_skills=3)
        print(json.dumps({"autolearn": made}, ensure_ascii=False, indent=2))
        return 0

    maybe = _skills_ops(args, fs=fs)
    if maybe is not None:
        return maybe
    maybe2 = _perception_ops(args, fs=fs)
    if maybe2 is not None:
        return maybe2

    if args.analyze_file:
        return _run_engineer_on_file(
            args.analyze_file,
            nano=args.nano,
            no_autofix=args.no_autofix,
            iters=args.iters,
            target_score=args.target_score,
            time_limit=args.timeout,
            max_output_bytes=args.max_bytes,
        )

    task: str | None = args.task
    if args.interactive and not task:
        try:
            task = input("Enter task: ").strip()
        except EOFError:
            task = None
    if not task:
        task = "Default: Activate learning curriculum and run engineer cycle"
        print("[note] No task provided; using default task.")

    mem = _make_memory(fs)
    eng = CognitiveEngine(
        memory=mem,
        config=CognitiveConfig(
            time_limit=args.timeout,
            max_output_bytes=args.max_bytes,
            verbose=args.verbose,
        ),
    )
    skills_dir = args.skills_dir or fs.join("skills")
    sm = SkillManager(eng.tools, skills_dir, constitution, fs)
    sm.load_all_and_register()
    _ensure_fallback_ocr(eng.tools, fs)

    use_autolearn = (
        constitution.allow_autolearn if args.autolearn is None else args.autolearn
    )

    mem.store(
        kind="note",
        payload={
            "text": "The system serves the wellbeing of Zurab and Nano Shengelia."
        },
    )
    ep = eng.run(
        task,
        goals=[
            Goal(text="Activate learning curriculum"),
            Goal(text="Run engineer cycle"),
        ],
    )
    print("\n========== EPISODE TRACE ==========")
    print(ep.text())
    print("\nFinished:", ep.finished_at)

    if use_autolearn:
        try:
            al = AutoLearner(mem, sm, fs, constitution)
            made = al.propose_and_build(max_skills=3)
            print(
                "\n[autolearn] results:", json.dumps(made, ensure_ascii=False, indent=2)
            )
        except Exception as e:
            print("[autolearn] failed:", e)

    return 0

def main_cli() -> int:
    import sys
    code = 0
    try:
        code = main(sys.argv[1:])
    except SystemExit as e:
        code = e.code
    if isinstance(code, int) and code is not None:
        return code
    return 0

if __name__ == "__main__":
    try:
        code = main(sys.argv[1:])
    except SystemExit as e:
        code = e.code
    if isinstance(code, int) and code != 0:
        sys.exit(code)
